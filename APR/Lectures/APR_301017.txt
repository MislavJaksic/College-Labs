2.2.5 Postupak po Powellu

Prilagodljiv nacin pretrazivanja po smjeru koji netrebaju biti koordinatne osi.

4-19 u skripti, govori o svojstvima kvadratnih funkcija.

Najveca potencija je 2.
F(x) = c + bT*x + 0.5*xT*A*x

Primjer: F(x) = (x1 + 2)**2 + (x2 + 1)**2
  - kada se izmnozi moze se napisati u kanonskom obliku:
    F(x) = 5 + [-4 2]*[[x1][x2]] + 0*5*[x1 x2]*[[2 0][0 2]]*[[x1][x2]]

Uvjet za ekstrem kvadratne funkcije: nablaF = b + A*x
  - A je Hesseova matrica (sadrzi sve parcijalne dvostruke derivacije)

Minimum kvadratne funkcije: xmin = A**(-1)*b

Ako je A pozitivno definitna, onda je xmin sigurno minimum.
Ako je negativno definitna, onda ce xmin biti maximum.
Ako je niti jedno niti drugo, onda ce xmin biti "sedlo".


Powell trazi minimum u konjugiranim smjerovima.
"vi" i "vj" smjerovi su konjugirani s obzirom na matricu A akko: vi*A*vj = 0
  - ako je A pozitivno definitna onda vrijedi i: vi*A*vi > 0

1) Minimum kvadratne funckije dobiva se optimiranjem u n-konjugiranih smjerova s obzirom na matricu A.
     Krenimo iz x0 i nademo minimum na pravcu -> x1 = x0 + lambda1*v1
     x2 = x1 + lambda2*v2, gdje je v2 konjugiran naspram v1
     To radimo za svaku dimenziju. -> xn = x0 + SUM(lambda[j+1]*v[j+1]) 
     xn je minimum.

2) Novi konjugirani smjer dobiva se spajanjem minimuma na dva paralelna pravca zadanog smjera.
     Dodatno, gradijent ce uvijek biti okomit na 

Algoritam gradi konjugirane smjerova i ponasa se kao da je funkcija kvadratna. Pretraga po konjugiranim smjerovaima ce nam dopustiti da brze dodemo do minimuma.

Ako je funkcija kvadratna, sigurno ce doci do minimuma u n koraka. Ako nije kvadratna, onda ce se ponasati drugacije.

Trebamo znati svojstva kvadratnih funkcija, na cemu se temelji Powell i kako Powell gradi konjugirane smjerova.

2.3. Optimiranje uz uporabu derivacija (gradijentni postupci)

2.3.1 Najbrzi spust

v = (-1*)nablaF / ||nablaF|| je smjer najbrzeg spusta
  Idi u negativnom smjeru gradijenta.

Globalno geldano je losa metoda: ako je funkcija kvadratna, onda kad dode do minimuma pravca, on ce upravo biti okomit na smjer kretanja. To ce kao posljedicu imati da cemo ici cik cak.
                                 osjetljivost na odredivanje minimuma na pravcu (ostar rub ili spica).
                                 
2.3.2 Newton Raphsonove metode

Za kvadratne funkcije uvijek vodi do minimuma u prvom koraku. Za ostale je iterativan.

Moguci problem je nestabilnost postupka (divergencija).

Tehnike povecanja stabilnosti:
  a) trazenje minimuma na pravcu - ne idi za deltaX, vec na tom pravcu trazi minimum
  b) smanjivanje koraka - nemoj ici za cijeli deltaX, vec za neki dijelic njega
  c) ako stvar divergira, koristi najbrzi spust

Racunalno zahtjevna: treba izracunati druge parcijalne derivacije i invertirati Hesseova matricu.

2.3.3 Fletcher-Powell (predstavnik kvazi Newtonovih metoda)

State-of-the-art optimizacija! Ako jedino imamo gradijent funkcije cilja, nema bolje.

Hesseova matrica, odnosno njena inverzija se aproksimira tijekom provedbe postupka. 
  - takva matrica je G ~ H*(-1)
  - v[j+1] = - G*nablaF, funkcija se optimira u zadanom smjeru

deltaG[j]*deltag[j] = deltax[j] - G[j]*deltax[j]
  kaj rezultira deltaG[j] = ...
  
Up prvoj iteraciji v1 = (-1)*nablaF ali u sljedecima je drugacije.

G[n] = E + SUM(M) - SUM(N)

M[n] = A**(-1)
N[n] = E <- PAZI! To ne pise u skripti koju imas!

Forumla Fletchera i Powella jamci da ce za kvadratnu funkciju aproksimacija G biti jednaka inverziji Hesseove matrici.

Neke durge kvazi Newtonove metode: y i z vektori su malo drugaciji, ali su svi postupci slicni.

U MatLabu -> fminunc (minimize unconstrained)
