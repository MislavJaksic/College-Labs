## Linearni diskriminativni modeli

Granica izmedu klasa je linearna. Hiperravnine.

h(X;w) = wT*X -> rezultat je realni broj

h(X;w) = f(wT*X= -> rezultat je broj izmedu [0,1] ili [-1,1]

Funkcija daje ili pozitivan ili negativan broj.
Ulazni prostor se zapravo dijeli na negativne i pozitivne dijelove ravnine.

Granica je uvijek za jednu dimenziju manja od prostora kojeg dijeli.
  - pravac za 2D, ravnina za 3D, hiperravnina za 4D itd...

Granica = Diskriminativna funkcija = Decizijska funkcija

Ako zelimo nelinearnu granicu: preslikaj s phi() x-eve u vecu dimenziju.
  - kao i kod regresije
PAZI! Takav model je i dalje linearan obzirom na parametre jer se rijec "linearan" odnosi na linearnost parametara wi.

Generativni modeli: imaju vise parametara i modeliraju vise od same granice.
                    oni modeliraju vjerojatnosti i time onda impliciraju granicu izmedu primjera.

K = 2 -> postoje dvije klase
h(X,w) = wT*X
h(X,w0,wi) = w0 + wT*X
h(x) = 0 -> granica (n-1 dimenzijski podprostor u n-dimenzijskom prostoru)

n = 2 (dvije dimenzije, znacajke)
h(x,w0,wi) = w0 + w1*x1 + w2*x2

||w|| je norma -> ||w|| = sqrt(w1**2 + w2**2 + ... + wn**2)
               -> ||w|| = sqrt(wT*w)

1)
w je zapravo normala na hiperravninu. 

h(x1) = h(x2) = 0 ako x1 i x2 leze na granici

w0 + wT*x1 = w0 + WT*x2

w(x1 - x2) = 0

Ako se oduzmu x1 i x2, tocke koje leze na granici, onda se dobije vektor paralelan s granicom koji ujedno i lezi na njoj.

2)
d je zapravo udaljenost hiperravnine od ishodista.

xT*x + w0 = 0 / (||w||)
wT*Xx/(||w||) = -w0/(||w||)
wT*x/(||w||) je projekcija vektora X na vektor w.

d = -w0/(||w||)

3)
Koliko je tocka x udaljena od hiperravnine? Ako je blizu, onda nismo sigurni da smo dobro klasificirali.

d = h(x)/(||w||)

Vektor tezine:                        (w0, wi)
Vektor normale:                       w
Udaljenost hiperravnine of ishodista: -w0/||w||
Udaljenost tocke od hipperravnine:    h(X)/||w||



Viseklasna klasifikacija: K > 2
                          kako iskoristiti binarni klasifikator za rjesavanje viseklasne klasifikacije?
                            - za svaka dva para klasa radimo binarnu klasifikaciju

One-vs-one: OVO, jedan naspram jedan
            potrebno je (K povrh 2) binarnih modela
            svaki binarni model ima jedan glas
              - glasanjem odredimo pripadnost primjera klasi
              - unutar "Bermudskog n-terokuta" nema rjesenja jer glasanje rezultira nerjesenim rezultatom
h(x) = argmax SUM(sgn())

One-vs-rest: OVR, jedan naspram ostali
             potrebno je K binarnih modela
             klasifikacija ovisno o blizini (udaljenosti) primjera granicama
               - tocno na "Bermudskim (tankim) ravninama" nema rjesenja zbog ekvidistance

OVR ima manje modela od OVO, pa je brzi, ali OVO je sigurniji.
OVR se moze dovesti do disbalansa zbog uravnotezenosti klasa.



(Visestruka) klasifikacija pomocu regresije:

Funkcija pogreske: E(w|D) = ...
Optimizacija: w = ... kao i u regresiji

K = 2
n = 2
Ideja: napravi hipotezu koja svrstava tocke u vrijednost izmedu 0 i 1. Ako je > 0.5, onda je hipoteza predvidjela pripadnost jednoj klasi, inace je predvidjela pripadnost onoj drugoj.
Svi iz jedne klasu imaju vrijednost 0, a svi druge klase imaju vrijednost 1.

Kada imas primjere, onda prilikom klasifikacije regresijom one koji su unutar zeljene klase dajes vrijednost y=1, a svima koji nisu u zeljenoj klasi daj vrijednost y=0. Zatim kada se trenira hipoteze za regresiju, povuci ce se pravac koji ce predvidati vrijednost h(x) gdje ako h(x) > 0.5, onda se predvida da primjer pripada zeljenoj klasi.

K > 2
Ista shema kao i za K = 2...

Veliki nedostaci klasifikacije regresijom: izlazi nisu vjerojatnosti, vec su nekakvi brojevi i najgore od svega, model je nerobusan na primjere koji odskacu (oni ce povoci pravac prema sebi i iskriviti hiperravninu tako da pogresno klasificira primjere koji ne odskacu).

Uzrok: funkcija najmanjih kvadrata jednako kaznjava sve primjere (i one koji su dobro klasificirani)

Perceptron: imas skicu u biljeznici.
