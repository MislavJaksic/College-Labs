{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sveučilište u Zagrebu  \n",
    "Fakultet elektrotehnike i računarstva  \n",
    "  \n",
    "## Strojno učenje 2017/2018  \n",
    "http://www.fer.unizg.hr/predmet/su"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "\n",
    "### Laboratorijska vježba 1: Regresija\n",
    "\n",
    "*Verzija: 1.1  \n",
    "Zadnji put ažurirano: 16. listopada 2017.*\n",
    "\n",
    "(c) 2015-2017 Jan Šnajder, Domagoj Alagić, Mladen Karan \n",
    "\n",
    "Objavljeno: **16. listopada 2017.**  \n",
    "Rok za predaju: **23. listopada 2017. u 07:00h**\n",
    "\n",
    "------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upute\n",
    "\n",
    "Prva laboratorijska vježba sastoji se od osam zadataka. U nastavku slijedite upute navedene u ćelijama s tekstom. Rješavanje vježbe svodi se na **dopunjavanje ove bilježnice**: umetanja ćelije ili više njih **ispod** teksta zadatka, pisanja odgovarajućeg kôda te evaluiranja ćelija. \n",
    "\n",
    "Osigurajte da u potpunosti **razumijete** kôd koji ste napisali. Kod predaje vježbe, morate biti u stanju na zahtjev asistenta (ili demonstratora) preinačiti i ponovno evaluirati Vaš kôd. Nadalje, morate razumjeti teorijske osnove onoga što radite, u okvirima onoga što smo obradili na predavanju. Ispod nekih zadataka možete naći i pitanja koja služe kao smjernice za bolje razumijevanje gradiva (**nemojte pisati** odgovore na pitanja u bilježnicu). Stoga se nemojte ograničiti samo na to da riješite zadatak, nego slobodno eksperimentirajte. To upravo i jest svrha ovih vježbi.\n",
    "\n",
    "Vježbe trebate raditi **samostalno**. Možete se konzultirati s drugima o načelnom načinu rješavanja, ali u konačnici morate sami odraditi vježbu. U protivnome vježba nema smisla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "# Učitaj osnovne biblioteke...\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadatci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Jednostavna regresija"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zadan je skup primjera $\\mathcal{D}=\\{(x^{(i)},y^{(i)})\\}_{i=1}^4 = \\{(0,4),(1,1),(2,2),(4,5)\\}$. Primjere predstavite matrixom $\\mathbf{X}$ dimenzija $N\\times n$ (u ovom slučaju $4\\times 1$) i vektorom oznaka $\\textbf{y}$, dimenzija $N\\times 1$ (u ovom slučaju $4\\times 1$), na sljedeći način:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array([[0],[1],[2],[4]])\n",
    "y = np.array([4,1,2,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a)\n",
    "\n",
    "Proučite funkciju [`PolynomialFeatures`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) iz biblioteke `sklearn` i upotrijebite je za generiranje matrice dizajna $\\mathbf{\\Phi}$ koja ne koristi preslikavanje u prostor više dimenzije (samo će svakom primjeru biti dodane *dummy* jedinice; $m=n+1$).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PolynomialTransformation(X, degree):\n",
    "    features = PolynomialFeatures(degree)\n",
    "    return features.fit_transform(X)\n",
    "\n",
    "print X\n",
    "degree = 1 # if degree > 1, construct n-degree polynomial\n",
    "XPoly1 = PolynomialTransformation(X, degree)\n",
    "print XPoly1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upoznajte se s modulom [`linalg`](http://docs.scipy.org/doc/numpy/reference/routines.linalg.html). Izračunajte težine $\\mathbf{w}$ modela linearne regresije kao $\\mathbf{w}=(\\mathbf{\\Phi}^\\intercal\\mathbf{\\Phi})^{-1}\\mathbf{\\Phi}^\\intercal\\mathbf{y}$. Zatim se uvjerite da isti rezultat možete dobiti izračunom pseudoinverza $\\mathbf{\\Phi}^+$ matrice dizajna, tj. $\\mathbf{w}=\\mathbf{\\Phi}^+\\mathbf{y}$, korištenjem funkcije [`pinv`](http://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.pinv.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import linalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ManualLinReg(X, y):\n",
    "    XT = X.transpose()\n",
    "    XTmulX = np.dot(XT, X)\n",
    "    XTmulXinv = np.linalg.inv(XTmulX)\n",
    "    w = np.dot(np.dot(XTmulXinv, XT), y)\n",
    "    return w\n",
    "\n",
    "w = ManualLinReg(XPoly1, y)\n",
    "print w\n",
    "\n",
    "w = np.dot(np.linalg.pinv(XPoly1), y)\n",
    "print w\n",
    "\n",
    "#w = w.transpose()\n",
    "#print w #the transpose has happened but it doesn't effect the repr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Radi jasnoće, u nastavku je vektor $\\mathbf{x}$ s dodanom *dummy* jedinicom $x_0=1$ označen kao $\\tilde{\\mathbf{x}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prikažite primjere iz $\\mathcal{D}$ i funkciju $h(\\tilde{\\mathbf{x}})=\\mathbf{w}^\\intercal\\tilde{\\mathbf{x}}$. Izračunajte pogrešku učenja prema izrazu $E(h|\\mathcal{D})=\\frac{1}{2}\\sum_{i=1}^N(\\tilde{\\mathbf{x}}^{(i)} - h(\\tilde{\\mathbf{x}}))^2$. Možete koristiti funkciju srednje kvadratne pogreške [`mean_squared_error`]( http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error) iz modula [`sklearn.metrics`](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics).\n",
    "\n",
    "**Q:** Gore definirana funkcija pogreške $E(h|\\mathcal{D})$ i funkcija srednje kvadratne pogreške nisu posve identične. U čemu je razlika? Koja je \"realnija\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = np.dot(XPoly1, w)\n",
    "print y\n",
    "print predictions\n",
    "\n",
    "print mean_squared_error(y, predictions)\n",
    "print mean_squared_error(y, predictions) * len(X)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uvjerite se da za primjere iz $\\mathcal{D}$ težine $\\mathbf{w}$ ne možemo naći rješavanjem sustava $\\mathbf{w}=\\mathbf{\\Phi}^{-1}\\mathbf{y}$, već da nam doista treba pseudoinverz.\n",
    "\n",
    "**Q:** Zašto je to slučaj? Bi li se problem mogao riješiti preslikavanjem primjera u višu dimenziju? Ako da, bi li to uvijek funkcioniralo, neovisno o skupu primjera $\\mathcal{D}$? Pokažite na primjeru."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print np.linalg.inv(X) #cannot do that. The matrix isn't a square matrix.\n",
    "XHighD = np.array([[0,1,2,3],\n",
    "                   [1,5,1,1],\n",
    "                   [2,1,1,1],\n",
    "                   [4,1,1,1],  \n",
    "                  ])\n",
    "XHighDinv = np.linalg.inv(XHighD)\n",
    "print XHighDinv\n",
    "print np.dot(XHighDinv,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (e) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proučite klasu [`LinearRegression`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression) iz modula [`sklearn.linear_model`](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model). Uvjerite se da su težine koje izračunava ta funkcija (dostupne pomoću atributa `coef_` i `intercept_`) jednake onima koje ste izračunali gore. Izračunajte predikcije modela (metoda `predict`) i uvjerite se da je pogreška učenja identična onoj koju ste ranije izračunali."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PerformModel(model, X, y):\n",
    "    model.fit(X, y)\n",
    "    w0 = model.intercept_\n",
    "    wi = model.coef_\n",
    "    predictions = model.predict(X)\n",
    "    trainError = mean_squared_error(predictions, y)\n",
    "    return w0, wi, predictions, trainError\n",
    "    \n",
    "model = LinearRegression()\n",
    "w0, wi, predictions, trainError = PerformModel(model, XPoly1, y)\n",
    "print w0\n",
    "print wi\n",
    "print predictions\n",
    "print trainError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Polinomijalna regresija i utjecaj šuma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a)\n",
    "\n",
    "Razmotrimo sada regresiju na većem broju primjera. Koristite funkciju `make_labels(X, f, noise=0)` koja uzima matricu neoznačenih primjera $\\mathbf{X}_{N\\times n}$ te generira vektor njihovih oznaka $\\mathbf{y}_{N\\times 1}$. Oznake se generiraju kao $y^{(i)} = f(x^{(i)})+\\mathcal{N}(0,\\sigma^2)$, gdje je $f:\\mathbb{R}^n\\to\\mathbb{R}$ stvarna funkcija koja je generirala podatke (koja nam je u stvarnosti nepoznata), a $\\sigma$ je standardna devijacija Gaussovog šuma, definirana parametrom `noise`. Za generiranje šuma koristi se funkcija [`numpy.random.normal`](http://docs.scipy.org/doc/numpy/reference/generated/numpy.random.normal.html). \n",
    "\n",
    "Generirajte skup za učenje od $N=50$ primjera uniformno distribuiranih u intervalu $[-5,5]$ pomoću funkcije $f(x) = 5 + x -2 x^2 -5 x^3$ uz šum  $\\sigma=200$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy.random import normal\n",
    "from random import uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# N \"tall\"\n",
    "# n \"wide\"\n",
    "# noise is standard deviation, sigma\n",
    "\n",
    "def MakeLabels(X, f, noise=0) :\n",
    "    if noise > 0:\n",
    "        return np.array([(f(x) + np.random.normal(0, noise)) for x in X])\n",
    "    return  np.array([f(x) for x in X])\n",
    "\n",
    "def Function(x):\n",
    "    return 5 + x - 2*x*x - 5*x*x*x\n",
    "\n",
    "XUniform2 = np.array([uniform(-5,5) for number in xrange(50)])\n",
    "XUniform2.sort()\n",
    "XUniform2 = XUniform2.reshape(-1, 1)\n",
    "f = Function\n",
    "y2 = MakeLabels(XUniform2, f, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prikažite taj skup funkcijom [`scatter`](http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.scatter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def DrawFunctionOnInterval(f, interval):\n",
    "    XDraw = sp.linspace(interval[0], interval[1])\n",
    "    yDraw = MakeLabels(XDraw, f, 0)\n",
    "    plt.plot(XDraw, yDraw, \"r--\")\n",
    "    \n",
    "def DrawUniformExamples(X, y):\n",
    "    plt.scatter(X, y)\n",
    "\n",
    "DrawUniformExamples(XUniform2, y2)\n",
    "DrawFunctionOnInterval(f, (-5,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trenirajte model polinomijalne regresije stupnja $d=3$. Na istom grafikonu prikažite naučeni model $h(\\mathbf{x})=\\mathbf{w}^\\intercal\\tilde{\\mathbf{x}}$ i primjere za učenje. Izračunajte pogrešku učenja modela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def DrawResults(X, y, color):\n",
    "    plt.plot(X, y, color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "degree = 3 # if degree > 1, construct n-degree polynomial\n",
    "           # example: d=3 -> (1, x, x*x, x*x*x)\n",
    "features = PolynomialFeatures(degree)\n",
    "\n",
    "model = LinearRegression()\n",
    "XPoly3 = features.fit_transform(XUniform2)\n",
    "w0, wi, predictions, trainError = PerformModel(model, XPoly3, y2)\n",
    "\n",
    "print trainError\n",
    "DrawUniformExamples(XUniform2, y2)\n",
    "DrawFunctionOnInterval(f, (-5,5))\n",
    "DrawResults(XUniform2, predictions, \"g\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Odabir modela"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a)\n",
    "\n",
    "Na skupu podataka iz zadatka 2 trenirajte pet modela linearne regresije $\\mathcal{H}_d$ različite složenosti, gdje je $d$ stupanj polinoma, $d\\in\\{1,3,5,10,20\\}$. Prikažite na istome grafikonu skup za učenje i funkcije $h_d(\\mathbf{x})$ za svih pet modela (preporučujemo koristiti `plot` unutar `for` petlje). Izračunajte pogrešku učenja svakog od modela.\n",
    "\n",
    "**Q:** Koji model ima najmanju pogrešku učenja i zašto?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "\n",
    "degrees = [1,   3,   5,   10,  20]\n",
    "colors = [\"g\", \"b\", \"c\", \"m\", \"y\"]\n",
    "for index in xrange(len(degrees)):\n",
    "    degree = degrees[index]\n",
    "    features = PolynomialFeatures(degree)\n",
    "    \n",
    "    XPoly = features.fit_transform(XUniform2)\n",
    "    w0, wi, predictions, trainError = PerformModel(model, XPoly, y2)\n",
    "    \n",
    "    print \"Degree:\"+str(degree),\n",
    "    print \"trainError:\"+str(trainError)\n",
    "    color = colors[index]\n",
    "    plt.plot(XUniform2, predictions, color)\n",
    "    \n",
    "DrawUniformExamples(XUniform2, y2)\n",
    "DrawFunctionOnInterval(f, (-5,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Razdvojite skup primjera iz zadatka 2 pomoću funkcije [`cross_validation.train_test_split`](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html#sklearn.cross_validation.train_test_split) na skup za učenja i skup za ispitivanje u omjeru 1:1. Prikažite na jednom grafikonu pogrešku učenja i ispitnu pogrešku za modele polinomijalne regresije $\\mathcal{H}_d$, sa stupnjem polinoma $d$ u rasponu $d\\in [1,2,\\ldots,20]$. Radi preciznosti, funkcije $h(\\mathbf{x})$ iscrtajte na cijelom skupu primjera (ali pogrešku generalizacije računajte, naravno, samo na ispitnome skupu). Budući da kvadratna pogreška brzo raste za veće stupnjeve polinoma, umjesto da iscrtate izravno iznose pogrešaka, iscrtajte njihove logaritme.\n",
    "\n",
    "**NB:** Podjela na skupa za učenje i skup za ispitivanje mora za svih pet modela biti identična.\n",
    "\n",
    "**Q:** Je li rezultat u skladu s očekivanjima? Koji biste model odabrali i zašto?\n",
    "\n",
    "**Q:** Pokrenite iscrtavanje više puta. U čemu je problem? Bi li problem bio jednako izražen kad bismo imali više primjera? Zašto?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Decrication source](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html#sklearn.cross_validation.train_test_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#from sklearn import cross_validation #Depricated!\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PerformModelAndCrossValidation(model, X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    predictions = model.predict(X_train)\n",
    "    trainError = mean_squared_error(y_train, predictions)\n",
    "\n",
    "    predictions = model.predict(X_test)\n",
    "    testError = mean_squared_error(y_test, predictions)\n",
    "    \n",
    "    return trainError, testError\n",
    "        \n",
    "def DrawErrors(xAxis, trainErrors, testErrors, ax=False):\n",
    "    if ax:\n",
    "        ax.plot(xAxis, log(trainErrors), \"b\")\n",
    "        ax.plot(xAxis, log(testErrors), \"r\")\n",
    "    else:\n",
    "        plt.plot(xAxis, log(trainErrors), \"b\")\n",
    "        plt.plot(xAxis, log(testErrors), \"r\")\n",
    "\n",
    "model = LinearRegression()\n",
    "trainErrors = []\n",
    "testErrors = []\n",
    "degrees = xrange(1, 21)\n",
    "for degree in degrees:\n",
    "    features = PolynomialFeatures(degree)\n",
    "    \n",
    "    XPoly = features.fit_transform(XUniform2)\n",
    "    trainError, testError = PerformModelAndCrossValidation(model, XPoly, y2)\n",
    "    trainErrors.append(trainError)\n",
    "    testErrors.append(testError)\n",
    "    \n",
    "    print \"Degree:\"+str(degree),\n",
    "    print \"testError:\"+str(testError)\n",
    "\n",
    "DrawErrors(range(1, 21), trainErrors, testErrors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Točnost modela ovisi o (1) njegovoj složenosti (stupanj $d$ polinoma), (2) broju primjera $N$, i (3) količini šuma. Kako biste to analizirali, nacrtajte grafikone pogrešaka kao u 3b, ali za sve kombinacija broja primjera $N\\in\\{100,200,1000\\}$ i količine šuma $\\sigma\\in\\{100,200,500\\}$ (ukupno 9 grafikona). Upotrijebite funkciju [`subplots`](http://matplotlib.org/examples/pylab_examples/subplots_demo.html) kako biste pregledno posložili grafikone u tablicu $3\\times 3$. Podatci se generiraju na isti način kao u zadatku 2.\n",
    "\n",
    "**NB:** Pobrinite se da svi grafikoni budu generirani nad usporedivim skupovima podataka, na sljedeći način. Generirajte najprije svih 1000 primjera, podijelite ih na skupove za učenje i skupove za ispitivanje (dva skupa od po 500 primjera). Zatim i od skupa za učenje i od skupa za ispitivanje načinite tri različite verzije, svaka s drugačijom količinom šuma (ukupno 2x3=6 verzija podataka). Kako bi simulirali veličinu skupa podataka, od tih dobivenih 6 skupova podataka uzorkujte trećinu, dvije trećine i sve podatke. Time ste dobili 18 skupova podataka -- skup za učenje i za testiranje za svaki od devet grafova."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Q:*** Jesu li rezultati očekivani? Obrazložite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Ns = [100,200,1000]\n",
    "sigmas = [100,200,500]\n",
    "f, axarr = plt.subplots(3, 3, sharex='col', sharey='row')\n",
    "counter = 0\n",
    "for N in Ns:\n",
    "    for sigma in sigmas:\n",
    "        X3 = np.array([uniform(-5,5) for number in xrange(N)])\n",
    "        X3.sort()\n",
    "        X3 = X3.reshape(-1, 1)\n",
    "        f = Function\n",
    "        y3 = MakeLabels(X3, f, sigma)\n",
    "        \n",
    "        model = LinearRegression()\n",
    "        trainErrors = []\n",
    "        testErrors = []\n",
    "        degrees = xrange(1, 21)\n",
    "        for degree in degrees:\n",
    "            features = PolynomialFeatures(degree)\n",
    "\n",
    "            XPoly = features.fit_transform(X3)\n",
    "            trainError, testError = PerformModelAndCrossValidation(model, XPoly, y3)\n",
    "            trainErrors.append(trainError)\n",
    "            testErrors.append(testError)\n",
    "\n",
    "            print \"Degree:\"+str(degree),\n",
    "            print \"testError:\"+str(testError)\n",
    "        \n",
    "        DrawErrors(range(1,21), trainErrors, testErrors, ax=axarr[counter/3,counter%3])\n",
    "        axarr[counter/3,counter%3].set_title(\"N:\"+str(N)+\";sigma:\"+str(sigma))\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Regularizirana regresija"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a)\n",
    "\n",
    "U gornjim eksperimentima nismo koristili **regularizaciju**. Vratimo se najprije na primjer iz zadatka 1. Na primjerima iz tog zadatka izračunajte težine $\\mathbf{w}$ za polinomijalni regresijski model stupnja $d=3$ uz L2-regularizaciju (tzv. *ridge regression*), prema izrazu $\\mathbf{w}=(\\mathbf{\\Phi}^\\intercal\\mathbf{\\Phi}+\\lambda\\mathbf{I})^{-1}\\mathbf{\\Phi}^\\intercal\\mathbf{y}$. Napravite izračun težina za regularizacijske faktore $\\lambda=0$, $\\lambda=1$ i $\\lambda=10$ te usporedite dobivene težine.\n",
    "\n",
    "**Q:** Kojih je dimenzija matrica koju treba invertirati?\n",
    "\n",
    "**Q:** Po čemu se razlikuju dobivene težine i je li ta razlika očekivana? Obrazložite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X1 = np.array([[0],[1],[2],[4]])\n",
    "y1 = np.array([4,1,2,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "degree = 3\n",
    "features = PolynomialFeatures(degree)\n",
    "XPoly3 = features.fit_transform(X1)\n",
    "print XPoly3\n",
    "\n",
    "XT = XPoly3.transpose()\n",
    "XTmulX = np.dot(XT, XPoly3)\n",
    "\n",
    "I = np.eye(len(XPoly3))\n",
    "\n",
    "lams = [0,1,10]\n",
    "for lam in lams:\n",
    "    XTmulXinv = np.linalg.inv(XTmulX + np.dot(lam, I))\n",
    "    w = np.dot(np.dot(XTmulXinv, XT), y1)\n",
    "    print \"Lam:\"+str(lam),\n",
    "    print w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proučite klasu [`Ridge`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge) iz modula [`sklearn.linear_model`](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model), koja implementira L2-regularizirani regresijski model. Parametar $\\alpha$ odgovara parametru $\\lambda$. Primijenite model na istim primjerima kao u prethodnom zadatku i ispišite težine $\\mathbf{w}$ (atributi `coef_` i `intercept_`).\n",
    "\n",
    "**Q:** Jesu li težine identične onima iz zadatka 4a? Ako nisu, objasnite zašto je to tako i kako biste to popravili."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Ridge Loss Function](http://scikit-learn.org/stable/modules/linear_model.html#ridge-regression)\n",
    "\n",
    "[Lecture Ridge Loss Function](http://www.fer.unizg.hr/_download/repository/SU-2016-4-Regresija2.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lams = [0,1,10]\n",
    "for lam in lams:\n",
    "    model = Ridge(alpha=lam)\n",
    "    w0, wi, predictions, trainError = PerformModel(model, XPoly3, y1)\n",
    "    print w0\n",
    "    print wi\n",
    "#??? Why is there a difference?."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Regularizirana polinomijalna regresija"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a)\n",
    "\n",
    "Vratimo se na slučaj $N=50$ slučajno generiranih primjera iz zadatka 2. Trenirajte modele polinomijalne regresije $\\mathcal{H}_{\\lambda,d}$ za $\\lambda\\in\\{0,100\\}$ i $d\\in\\{2,10\\}$ (ukupno četiri modela). Skicirajte pripadne funkcije $h(\\mathbf{x})$ i primjere (na jednom grafikonu; preporučujemo koristiti `plot` unutar `for` petlje).\n",
    "\n",
    "**Q:** Jesu li rezultati očekivani? Obrazložite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DrawUniformExamples(XUniform2, y2)\n",
    "DrawFunctionOnInterval(f, (-5,5))\n",
    "\n",
    "lams = [0,100]\n",
    "degrees = [2,10]\n",
    "colors = [\"g\", \"b\", \"c\", \"m\"]\n",
    "patches = []\n",
    "counter = 0\n",
    "for lam in lams:\n",
    "    for degree in degrees:\n",
    "        model = Ridge(alpha=lam)\n",
    "        features = PolynomialFeatures(degree)\n",
    "    \n",
    "        XPoly = features.fit_transform(XUniform2)\n",
    "        w0, wi, predictions, trainError = PerformModel(model, XPoly, y2)\n",
    "        \n",
    "        color = colors[counter]\n",
    "        counter += 1\n",
    "        \n",
    "        patches.append(mpatches.Patch(color=color, label=\"L:\"+str(lam)+\",d:\"+str(degree)))\n",
    "        plt.plot(XUniform2, predictions, color)\n",
    "        \n",
    "plt.legend(handles=patches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b)\n",
    "\n",
    "Kao u zadataku 3b, razdvojite primjere na skup za učenje i skup za ispitivanje u omjeru 1:1. Prikažite krivulje logaritama pogreške učenja i ispitne pogreške u ovisnosti za model $\\mathcal{H}_{d=20,\\lambda}$, podešavajući faktor regularizacije $\\lambda$ u rasponu $\\lambda\\in\\{0,1,\\dots,50\\}$.\n",
    "\n",
    "**Q:** Kojoj strani na grafikonu odgovara područje prenaučenosti, a kojoj podnaučenosti? Zašto?\n",
    "\n",
    "**Q:** Koju biste vrijednosti za $\\lambda$ izabrali na temelju ovih grafikona i zašto?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "degree = 20\n",
    "features = PolynomialFeatures(degree)\n",
    "XPoly20 = features.fit_transform(XUniform2)\n",
    "\n",
    "trainErrors = []\n",
    "testErrors = []\n",
    "lams = range(51)\n",
    "for lam in lams:\n",
    "    model = Ridge(alpha=lam)\n",
    "    \n",
    "    trainError, testError = PerformModelAndCrossValidation(model, XPoly20, y2)\n",
    "    trainErrors.append(trainError)\n",
    "    testErrors.append(testError)\n",
    "    \n",
    "    print \"Lam:\"+str(lam),\n",
    "    print \"testError:\"+str(testError)\n",
    "\n",
    "DrawErrors(lams, trainErrors, testErrors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. L1-regularizacija i L2-regularizacija"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Svrha regularizacije jest potiskivanje težina modela $\\mathbf{w}$ prema nuli, kako bi model bio što jednostavniji. Složenost modela može se okarakterizirati normom pripadnog vektora težina $\\mathbf{w}$, i to tipično L2-normom ili L1-normom. Za jednom trenirani model možemo izračunati i broj ne-nul značajki, ili L0-normu, pomoću sljedeće funkcije:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nonzeroes(coef, tol=1e-6): \n",
    "    return len(coef) - len(coef[sp.isclose(0, coef, atol=tol)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ERROR = (0.1)**6\n",
    "\n",
    "def IsFloatEqual(floatOne, floatTwo):\n",
    "    if ((floatTwo-ERROR) <= floatOne) and (floatOne <= (floatTwo+ERROR)):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def LNorm(wRow, p):\n",
    "    wColumn = wRow.reshape(-1, 1)\n",
    "    if p == 0:\n",
    "        countZeros = 0\n",
    "        for value in wColumn:\n",
    "            if IsFloatEqual(value[0], 0.0):\n",
    "                countZeros += 1\n",
    "        return len(wColumn) - countZeros\n",
    "    \n",
    "    if p == 1:\n",
    "        sum = 0\n",
    "        for value in wColumn:\n",
    "            sum += sqrt(value[0]**2)\n",
    "        return sum\n",
    "    \n",
    "    if p == 2:\n",
    "        try:\n",
    "            return sqrt(np.dot(wRow, wColumn)[0][0])\n",
    "        except:\n",
    "            return sqrt(np.dot(wRow, wColumn)[0])\n",
    "    \n",
    "    print \"ERROR_NOT_IMPLEMENTED\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a)\n",
    "\n",
    "Za ovaj zadatak upotrijebite skup za učenje i skup za testiranje iz zadatka 3b. Trenirajte modele **L2-regularizirane** polinomijalne regresije stupnja $d=20$, mijenjajući hiperparametar $\\lambda$ u rasponu $\\{1,2,\\dots,100\\}$. Za svaki od treniranih modela izračunajte L{0,1,2}-norme vektora težina $\\mathbf{w}$ te ih prikažite kao funkciju od $\\lambda$.\n",
    "\n",
    "**Q:** Objasnite oblik obiju krivulja. Hoće li krivulja za $\\|\\mathbf{w}\\|_2$ doseći nulu? Zašto? Je li to problem? Zašto?\n",
    "\n",
    "**Q:** Za $\\lambda=100$, koliki je postotak težina modela jednak nuli, odnosno koliko je model rijedak?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PerformLAndCalculateNorms(model, X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    wi = model.coef_\n",
    "    \n",
    "    L0 = LNorm(wi, 0)\n",
    "    L1 = LNorm(wi, 1)\n",
    "    L2 = LNorm(wi, 2)\n",
    "    \n",
    "    return L0, L1, L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L0s = []\n",
    "L1s = []\n",
    "L2s = []\n",
    "lams = range(1, 101)\n",
    "for lam in lams:\n",
    "    model = Ridge(alpha=lam)\n",
    "    \n",
    "    L0, L1, L2 = PerformLAndCalculateNorms(model, XPoly20, y2)\n",
    "    L0s.append(L0)\n",
    "    L1s.append(L1)\n",
    "    L2s.append(L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DrawResults(lams, L0s, \"g\")\n",
    "DrawResults(lams, L1s, \"y\")\n",
    "DrawResults(lams, L2s, \"c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Glavna prednost L1-regularizirane regresije (ili *LASSO regression*) nad L2-regulariziranom regresijom jest u tome što L1-regularizirana regresija rezultira **rijetkim modelima** (engl. *sparse models*), odnosno modelima kod kojih su mnoge težine pritegnute na nulu. Pokažite da je to doista tako, ponovivši gornji eksperiment s **L1-regulariziranom** regresijom, implementiranom u klasi  [`Lasso`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) u modulu [`sklearn.linear_model`](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L0s = []\n",
    "L1s = []\n",
    "L2s = []\n",
    "lams = range(1, 101)\n",
    "for lam in lams:\n",
    "    model = Lasso(alpha=lam)\n",
    "    \n",
    "    L0, L1, L2 = PerformLAndCalculateNorms(model, XPoly20, y2)\n",
    "    L0s.append(L0)\n",
    "    L1s.append(L1)\n",
    "    L2s.append(L2)\n",
    "    \n",
    "DrawResults(lams, L0s, \"g\")\n",
    "DrawResults(lams, L1s, \"y\")\n",
    "DrawResults(lams, L2s, \"c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Značajke različitih skala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Često se u praksi možemo susreti sa podatcima u kojima sve značajke nisu jednakih magnituda. Primjer jednog takvog skupa je regresijski skup podataka `grades` u kojem se predviđa prosjek ocjena studenta na studiju (1--5) na temelju dvije značajke: bodova na prijamnom ispitu (1--3000) i prosjeka ocjena u srednjoj školi. Prosjek ocjena na studiju izračunat je kao težinska suma ove dvije značajke uz dodani šum.\n",
    "\n",
    "Koristite sljedeći kôd kako biste generirali ovaj skup podataka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_data_points = 500\n",
    "np.random.seed(69)\n",
    "\n",
    "# Generiraj podatke o bodovima na prijamnom ispitu koristeći normalnu razdiobu i ograniči ih na interval [1, 3000].\n",
    "exam_score = np.random.normal(loc=1500.0, scale = 500.0, size = n_data_points) \n",
    "exam_score = np.round(exam_score)\n",
    "exam_score[exam_score > 3000] = 3000\n",
    "exam_score[exam_score < 0] = 0\n",
    "\n",
    "# Generiraj podatke o ocjenama iz srednje škole koristeći normalnu razdiobu i ograniči ih na interval [1, 5].\n",
    "grade_in_highschool = np.random.normal(loc=3, scale = 2.0, size = n_data_points)\n",
    "grade_in_highschool[grade_in_highschool > 5] = 5\n",
    "grade_in_highschool[grade_in_highschool < 1] = 1\n",
    "\n",
    "# Matrica dizajna.\n",
    "grades_X = np.array([exam_score,grade_in_highschool]).T\n",
    "\n",
    "# Završno, generiraj izlazne vrijednosti.\n",
    "rand_noise = np.random.normal(loc=0.0, scale = 0.5, size = n_data_points)\n",
    "exam_influence = 0.9\n",
    "grades_y = ((exam_score / 3000.0) * (exam_influence) + (grade_in_highschool / 5.0) \\\n",
    "            * (1.0 - exam_influence)) * 5.0 + rand_noise\n",
    "grades_y[grades_y < 1] = 1\n",
    "grades_y[grades_y > 5] = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iscrtajte ovisnost ciljne vrijednosti (y-os) o prvoj i o drugoj značajki (x-os). Iscrtajte dva odvojena grafa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vaš kôd ovdje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naučite model L2-regularizirane regresije ($\\lambda = 0.01$), na podacima `grades_X` i `grades_y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vaš kôd ovdje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sada ponovite gornji eksperiment, ali prvo skalirajte podatke `grades_X` i `grades_y`. Za tu svrhu, koristite [`StandardScaler`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Vaš kôd ovdje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Gledajući grafikone iz podzadatka (a), koja značajka bi trebala imati veću magnitudu, odnosno važnost pri predikciji prosjeka na studiju? Odgovaraju li težine Vašoj intuiciji? Objasnite.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Multikolinearnost i kondicija matrice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Izradite skup podataka `grades_X_fixed_colinear` tako što ćete u skupu `grades_X_fixed` iz\n",
    "zadatka 7b duplicirati zadnji stupac (ocjenu iz srednje škole). Time smo efektivno uveli savršenu multikolinearnost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vaš kôd ovdje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ponovno, naučite na ovom skupu L2-regularizirani model regresije ($\\lambda = 0.01$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vaš kôd ovdje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Usporedite iznose težina s onima koje ste dobili u zadatku *7b*. Što se dogodilo?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slučajno uzorkujte 50% elemenata iz skupa `grades_X_fixed_colinear` i naučite dva modela L2-regularizirane regresije, jedan s $\\lambda=0.01$, a jedan s $\\lambda=1000$. Ponovite ovaj pokus 10 puta (svaki put s drugim podskupom od 50% elemenata).  Za svaki model, ispišite dobiveni vektor težina u svih 10 ponavljanja te ispišite standardnu devijaciju vrijednosti svake od težina (ukupno šest standardnih devijacija, svaka dobivena nad 10 vrijednosti)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vaš kôd ovdje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Kako regularizacija utječe na stabilnost težina?  \n",
    "**Q:** Jesu li koeficijenti jednakih magnituda kao u prethodnom pokusu? Objasnite zašto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Koristeći [`numpy.linalg.cond`](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.linalg.cond.html) izračunajte kondicijski broj matrice $\\mathbf{\\Phi}^\\intercal\\mathbf{\\Phi}+\\lambda\\mathbf{I}$, gdje je $\\mathbf{\\Phi}$ matrica dizajna (`grades_fixed_X_colinear`). Ponovite i za $\\lambda=0.01$ i za $\\lambda=10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vaš kôd ovdje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Kako regularizacija utječe na kondicijski broj matrice $\\mathbf{\\Phi}^\\intercal\\mathbf{\\Phi}+\\lambda\\mathbf{I}$?  "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
