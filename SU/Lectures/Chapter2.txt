## Poglavlje 2: Nadzirano ucenje

Nadzirano ucenje: klasifikacija i regresija.

Klasifikacija: pridruzi klasu, diskretnu ili norminalnu vrijednost nekom primjeru.

Regresija: pridruzi kontinuiranu vrijednosti nekom primjeru.

### Osnovni pojmovi 

Dimenzija vektora n
Vektor znacajki (x1, x2, ..., xn)
Primjer x = (x1, x2, ..., xn), a.k.a. ulazni prostor/prostor primjera
Skup svih primjera X
Oznaka klase y

Klasa C

Ukupan broj primjera za ucenje N
Indeks primjera i
Primjer za ucenje (x, y)
Skup primjera za ucenje D = {xi, yi}, gdje i = (1 do N)
Broj klasa K
Vektor klasa (y1, y2, ..., yk)

Model ili prostor hipoteza H

Pretpostavka svih algoritama za strojno ucenje: primjeri u X su nezavisno uzrokovani i iz iste distribucije P(x, y).
                                                skraceno, idd (eng. independent and identically distributed).

Nadzirano ucenje: poznato je kojem x pripada koji y.

Klasifikacija: odreci C koji pripada x.
                 binarna
                 vise klasna (eng. multiclass classification)
                 s vise strukim oznakama (eng. multilabel classification)

Hipoteza: zadace klasifikacije je napraviti hipotezu i time odrediti pripada li x C-u.
            h(x)= 0 -> pripada C-u    -> tada x zadovoljava hipotezu
            h(x)= 1 -> ne pripada C-u  
          konzistentna s (x, y) akko h(x)= y, tj. ako algoritam predvidi kao sto pise u podacima.
          
Model: ili prostor hipoteza H je skup mogucih postupaka odlucivanja o pripadnosti x-a y-onu.
       ucenje se svodi na pretrazivanje H i nalazenje najbolje hipoteze. 
       koliko dobro h klasificira odreduje empirijska pogreska ili pogrska ucenja (eng. training error)
         greska = broj netocno klasificiranih primjera / ukupni broj primjera
         ako greska h je uvijek veca od 0, onda H nije dovoljnog kapaciteta ili slozenosti da bi naucio C
            
VC dimenzija: Vapnik-Chervonenkisova; iskazuje kapacitet H kao broj primjera s kojima se H moze nostiti
                razdjeljivanje (eng. shattering): kada H moze razdvojiti sve konfiguracije + i - primjera kojih je N u prostoru (konfiguracija je 2**N)
              VC(H) je najveci broj primjera koji H moze razdvojiti
               
### Induktivna pristranost

Generalizacija: svojstvo hipoteze da predvidi klasifikaciju nevidenog primjera

Induktivna pristranost: skup pretpostavka za induktivno ucenje.
                        1) definiramo H cime odredujemo koje hipoteze dolaze u obzir
                        2) definira se nacin odabira h iz H
                          - ogranicavanjem: odabirmo H ogranicimo moguc izbor h                       
                          - preferencijom: odaberemo nacin pretrazivanja H i time dajemo prednost nehim h nad nekim drugim
                        ono sto nedostaje indukciji da bi bila dedukcija
                        
Sum: anomalija u podacima
     onemogucuju pogresku da bude jednaka nuli
     
### Regresija

Regresija: ciljana vrijednost je kontinuirana.
           idealno se uci y = f(x), ali se zbog suma se uci y = f(x) + e
             - empirijska pogreska: 1/2 * SUM( (yi - h(xi))**2 ), ty kvadratna pogreska.
           
Linearna regresija: h(x) = w0 + SUM( w1*xi ) = w0 + w1*x1 + w2*x2 + ...
                      - wi su parametri
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    