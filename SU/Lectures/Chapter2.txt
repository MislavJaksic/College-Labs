## Poglavlje 2: Nadzirano ucenje

Nadzirano ucenje: klasifikacija i regresija.

Klasifikacija: pridruzi klasu, diskretnu ili norminalnu vrijednost primjeru.

Regresija: pridruzi kontinuiranu vrijednost primjeru.

### Osnovni pojmovi

Dimenzija vektora:  n
Vektor znacajki:    (x1, x2, ..., xn)
Primjer:            x = (x1, x2, ..., xn), a.k.a. 
Skup svih primjera: X tj. ulazni prostor/prostor primjera
Oznaka klase:       y
Skup oznaka:        Y

Klasa: C

Ukupan broj primjera za ucenje: N
Indeks primjera:                i
Primjer za ucenje:              (x, y)
Skup primjera za ucenje:        D = {xi, yi}, gdje i = (1 do N)
Broj klasa:                     K
Vektor klasa:                   (y1, y2, ..., yk)

Model ili prostor hipoteza: H

Pretpostavka svih algoritama za strojno ucenje: primjeri u X su nezavisno uzrokovani i iz iste distribucije P(x, y).
                                                  - skraceno, idd (eng. independent and identically distributed)

Nadzirano ucenje: poznato je kojem x pripada koji y.

Klasifikacija: odreci C koji pripada x.
               vrste: binarna,
                      vise klasna (eng. multiclass classification),
                      s vise strukim oznakama (eng. multilabel classification).

Hipoteza: zadace klasifikacije je napraviti hipotezu i time odrediti pripada li x C-u.
            - h(x)= 0 -> pripada C-u    -> tada x zadovoljava hipotezu
            - h(x)= 1 -> ne pripada C-u  
          konzistentna s (x, y) akko h(x)= y, tj. ako algoritam predvidi kao sto pise u podacima.
          
Model: ili prostor hipoteza H je skup mogucih postupaka odlucivanja o pripadnosti x-a y-onu.
       ucenje se svodi na pretrazivanje H i nalazenje najbolje hipoteze. 
       koliko dobro h klasificira odreduje empirijska pogreska ili pogrska ucenja (eng. training error).
         - greska = broj netocno klasificiranih primjera / ukupni broj primjera
         - ako greska h je uvijek veca od 0, onda H nije dovoljnog kapaciteta ili slozenosti da bi naucio C
            
VC dimenzija: Vapnik-Chervonenkisova; iskazuje kapacitet H kao broj primjera s kojima se H moze nostiti.
                - razdjeljivanje (eng. shattering): kada H moze razdvojiti sve konfiguracije + i - primjera kojih je N u prostoru (konfiguracija je 2**N)
              VC(H) je najveci broj primjera koji H moze razdvojiti.
               
### Induktivna pristranost

Generalizacija: svojstvo hipoteze da predvidi klasifikaciju nevidenog primjera.

Induktivna pristranost: skup pretpostavka za induktivno ucenje.
                        1) definiramo H cime odredujemo koje hipoteze dolaze u obzir,
                        2) definira se nacin odabira h iz H.
                          - ogranicavanjem: odabirmo H ogranicimo moguc izbor h                       
                          - preferencijom: odaberemo nacin pretrazivanja H i time dajemo prednost nehim h nad nekim drugim
                        ono sto nedostaje indukciji da bi bila dedukcija.
                        
Sum: anomalija u podacima.
     onemogucuju pogresku da bude jednaka nuli.
     
### Regresija

Regresija: ciljana vrijednost je kontinuirana.
           idealno se uci y = f(x), ali se zbog suma uci y = f(x) + e.
             - empirijska pogreska: 1/2 * SUM( (yi - h(xi))**2 ), kvadratna pogreska
               - iskazuje netocnost hipoteze h na D
           
Linearna regresija: h(x) = w0 + SUM( w1*xi ) = w0 + w1*x1 + w2*x2 + ...
                      - wi su parametri koje treba nauciti
                    ako zboj kvadrata pogresaka treba biti minimalan i h(x) = w0 + w1*x1 onda:
                      - postupak najmanjih kvadrata (eng. least squares) nalazi optimalnu h
                      
### Odabir modela (eng. model selection)

Odabir modela: svodi se na optimizaciju hiperparametara fiksnog modela.
               odabit modela == optimizacija hiperparametara.

Pazi! Svrha hipoteze je generalizacija, ne tocna klasifikacija primjera za ucenje.
Treba odabrati najjednostavniji model, ali ipak dovoljno kompliciran da ima veliku moc predvidanja.

Prenaucenost: (eng. overfitting) H je previse slozen, h imaju vise pretpostavaka nego postroji u primjerima, h su previse prilagodene sumu u podacima.
              modeli imaju veliku varijancu: male promjene u D dovode do velikih promjena hipoteza.
              
Podnaucenost: (eng. underfitting) H je prejednostavan, lose opisuje podatke iz D.
              modeli imaju malu varijancu: promjene u D se ne odrazavaju na hipoteze.
                                           velika pristranost (eng. bias) (u statistickom smislu).
                                           
Ideja: minimiziraj i varijancu i pristranost.

Unakrsna provjera: (eng. cross-validation) razdvaja D na skup za:
                     - ucenje (eng. training set) i
                     - provjeru/ispitivanje (eng. validation/testing set).
                       - skup je za "provjeru" ako se njime bira model, a zove se za "ispitivanje" ako se njime utvrduje pogreska vec odabranog modela.
                   od modela skrivamo dio primjera da vidimo kako ce se ponasati s nevidenim primjerima
                     - pogreska generalizacije (skup za provjeru) (eng. off-traning-set error) je greska hipoteze mjerena na skupu s kojim nije naucena.
                       - kako slozenost modela raste tako ona prvo pada, pa raste
                     - empirijska pogreska (skup za ucenje) (eng. train error) je greska 
                       - kako slozenost modela raste tako ona pada
                   optimalan model je onaj koji min(pogresku generalizacije)

Ako se unakrsna provjera koristi ne za odabir modela, vec za utvrdivanje konacne pogreske odabranog modela, skup s kojim se ispituje zove se ispitni skup (eng. test set).                   

Ispitni skup, skup za provjeru i ispitni skup trebaju biti disjunktni!

Ponasanje pogreske i pre/podnaucenost: prenaucenost ili visoka varijanca ocituje se u velikoj greski na skupu za provjeru.
                                       podnaucenost ili visoka pristranost ocituje se u velikoj greski na i skupu za provjeru i ispitnom skupu.

Drugi postupci odabira modela:  SRM, AIC, BIC, MDL.
                           
Komponente algoritma nadziranog ucenja:
Skup primjera za ucenje D.
Primjeri su iid.
Pronadi hipotezu h koja najbolje aproksimira yi.
Model ili prostor hipoteza H.
  - lakse predociti kao funkciju h(*|0) gdje su 0 parametri
  - H je skup funkcija h koju su parametrizirane s 0

Primjer 2.5

Funkcija gubitka (eng. loss function) L za parametre 0 izracunava razliku izmedu ciljane vrijednosti i aproksimacije.
  - iskazuje gresku na pojedinacnom primjeru (za razliku od empirijske pogreske)
  - kod regresije je najcesce kvadratna pogreska, kod klasifikacije je XXX VIDI PRIMJER
Empirijska pogreska je greska nad primjerima skupa za ucenje. VIDI DEFINICIJU
Matrica gubitka (eng. loss matrix) se definira kada nije svejedno jel je nastala lazno pozitivna ili negativna greska.
Optimizacijski postupcima nalazimo vrijednost parametara 0* za koje je empirijska pogreska najmanja.
  - taj postupak nije isti kao optimizacija slozenosti modela
  
Modeli strojnog ucenja razlikuju se po:
  1)modelu
  2)funkciji gubitka
  3)optimizacijskom postupku
  
### Pristupi nadziranom ucenju

