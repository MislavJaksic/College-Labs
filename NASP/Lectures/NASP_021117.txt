Krenuli sa opisom neurona kao sastavnim dijelom mreze.

U matematickom smislu to znaci odrediti one vrijednosti one tezine poveznice za koje ce neuron davati vrijednosti koje su najblize trazenim.

y = SUM(w*x) = wT*x
Minimiziraj 0.5*SUM(xT*w - y)**2 <- to znaci uvjezbati neuron

Minimiziraj ||XT*w - y||**2 = 0.5*||e||**2 <- donja ograda je 0

Pozitivno definitna matrica -> sigurno ima rjesenje.

Jedan neuron je zapravo rjesavanje linearne regresije: w = (xT*x)**(-1)*xT*y
A*x = b /* AT
AT*A*x = AT*b /* (AT*A)**(-1)
I*x = (AT*A)**(-1)*AT*y

Primjer:
x1 + x2 + x3 = 3
x1 + x2 +    = 2 <- beskonacno rjesenja, izaberi onu s najmanjom normom ||w|| (takav odgovor je bio i u SU_DZ2 )
Najmanja norma je jako vazna: tezine su pojacala izmedu neurona, zbog toga zelimo da su tezine w najmanje moguce.
  Ako su tezine prevelike (ili su izrazito neujednaceni), tehnicki bi to bilo neizvedivo s pojacalima.

A = [[1 1 1], b = [[3]
     [1 1 0]]      [2]]        
(A*AT)**(-1) = [[ 1 -1 ],
                [-1 3/2]]
X = [[0 1/2],   [[3]     
     [0 1/2],*** [2]     
     [1 -1 ]]      ]
X = [[1],
     [1],
     [1]]

Zvjezdica (*) kod w* oznacava optimalnu vrijednost. 

Kacmarcov algoritam: iteracijski se dolazi do rjesenja w*.
w[k+1] = w[k] - mi*(e[k]*x)/(||x||**2) <- +1 je zato jer indeksi matrice pocinju od 1
  R(k) = k MOD p, on drzi matricu unutar granice njezinih indeksa.

Primjer: onaj gore, ali Kacmarcovim algoritmom.
  Rjesen u biljeznici.

Sve do sada se odnosi na linearni neuron.


Kada bi postojala aktivacijska funkcija, neuron bi bio nelinearan.
Gradijentna metoda optimizacije:
f(x), f:R->R
x[k+1] = x[k] - alpha[k]*nablaf(x[k])
Ciljna funkcija za Adaline: f(w) = 0.5*||XT*w - y||**2
Gradijent ciljne funkcije: nablaf(w) = x[d]*(xT[d]*w - y[d]), gdje je iteracija napisana gore

Trebamo izracunati gradijent ciljne funkcije. Ona je zbroj svih kvadrata pogresaka. 
  e[i] je pogreska.
Skupno uvjezbavanje (eng. batch learning) je izracunavanje gradijenta sa svim uzorcima. Sve tezine se azuriraju istovremeno. 
nablaE(w) = nabla0.5*SUM(e[i]**2)

Mozemo dati samo jedan uzorak neuronu, izracunati gradijent i azurirati tezine.
Profesorova interpretacija formule: LMS algoritam. Opravdanje koracnog uvjezbavanja (eng. online learning)... i onda je rekao puno rijeci.
Uvjezbavanje jednog linearnog neurona: w[k+1] = w[k] - alpha[k]*x*e[k]



Uvjezbavanje cijele mreze
Troslojna mreza: bilo kakvu funkciju moguce je aproksimirati s njom. IZRAZITO VAZNO!
                 izlazni, skriveni i ulazni sloj.
Za ispit nam treba samo par slajdova s formulama koje trebamo upotrijebiti.

Ulazni sloj se crta kao da se tamo nalaze neuroni, ali oni su samo rasprsivaci signala.
Tezina poveznice izmedu j-tog i i-tog sloja prema skrivenom sloju (superskript h od hidden) je w[j,i]h.

Mrezi se na ulazu daju primjeri, a na izlazima se gledaju tezine koje mreza izbaci.
Znaj formulu na slajdu 37: w[k+1] = w[k] - alpha[k]*||e[i]||*nabla||e[i]||
Za velike mreze, to je bilo neizracunljivo do izuma algoritma rasprostiranja.

Algoritam rasprostiranja (prenosenja) unatrag (eng. backpropagation algorithm):
Promatrajuci zadnja dva sloja izveli su formulu za propagaciju.

Mora se znati sigmoida za ispit.
Sigmoida se stavlja u skrivene neurone. ("DSP guide" je zanimljiva knjiga o digitalnoj obradi signala)

Da bi na ispitu rjesili neuronsku mrezu kao zadatak trebamo znati samo slajd 60 i 61.
